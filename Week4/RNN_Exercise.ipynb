{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN and Natural Language Processing\n",
    "\n",
    "In this exercise we will try to classify IMDB dataset: Given the text of a review, can you predict if the review was positive or negative?\n",
    "\n",
    "Before doing this exercise, you might want to become more familier with LSTMs by considering the example FlightPassengerPredictions.\n",
    "\n",
    "The data for this exercise can be found here:\n",
    "https://sid.erda.dk/share_redirect/encok5nw3y\n",
    "\n",
    "***\n",
    "\n",
    "Author: Julius Kirkegaard and Troels C. Petersen<br>\n",
    "Date: 14th of May 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:20:56.142919Z",
     "end_time": "2023-05-17T11:20:57.949155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:22:28.661429Z",
     "end_time": "2023-05-17T11:22:32.439751Z"
    }
   },
   "outputs": [],
   "source": [
    "limit_data = 10000  # limit the amount of data for speed, change as you please\n",
    "\n",
    "def remove_special_symbols(string):\n",
    "    return ''.join(s for s in string if ord(s)>96 and ord(s)<123 or s == ' ')\n",
    "\n",
    "with open('train.json') as f:\n",
    "    train_text, train_labels = json.load(f)\n",
    "    idxs = np.random.permutation(len(train_text))\n",
    "    train_text = [remove_special_symbols(train_text[i]) for i in idxs[:limit_data]]\n",
    "    train_labels = [train_labels[i] for i in idxs[:limit_data]]\n",
    "    \n",
    "with open('test.json') as f:\n",
    "    test_text, test_labels = json.load(f)\n",
    "    idxs = np.random.permutation(len(test_text))   \n",
    "    test_text = [remove_special_symbols(test_text[i]) for i in idxs[:limit_data]]\n",
    "    test_labels = [test_labels[i] for i in idxs[:limit_data]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data... Here is a negative review (label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:22:42.361448Z",
     "end_time": "2023-05-17T11:22:42.395637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hilip  ickian movie nd a decent one for that matter etter than the aycheck oo and that abomination called inority eport pielberg ut lets face it the twisting and cheesing ending was a bit too much for me alf way through the movie  already started to fear about such kind of ending and  was regrettably right ut that does not mean that the film is not worth its time o not at all irst half as already many here have commented is awesome here are some parts where you start to doubt whether the director intended to convey the message that showmanship is highly important thing in the future we will do such kind on corny sf things because we  or is it simply over combining ut the paranoia is there and feeling out of joint also ood one\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_text[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a positive one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:22:56.440249Z",
     "end_time": "2023-05-17T11:22:56.466734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and amp was awful he aked ile was a little better and this third straight to  in the merican ie franchise seems the same quality as the predecessor asically rik tifler ohn hite split from his girlfriend after losing his virginity and now him and ike ooze oozeman ake iegel are joining riks cousin wight teve alley at college ith the promise of many parties plenty of booze and enough hot chicks at the eta ouse they only have fifty listed tasks to carry out to become official privileged members ut a threat comes into sight with the rivals  eek ouse led by powerhungry nerd and sheep shagger dgar yrone avage offering bigger and better than what eta have o settle it once and for all eta and ek go into battle with the banned for forty years reek ames to beat each other in with the loser moving out he last champion of the games oah evenstein aka ims ad the only regular ugene evy runs the show which sees the people unhooking bras a gladiator duel floating on water catching a greased pig ussian oulette in the mouth with cartridges of aged horse spunk wife carrying and drinking a full keg of alcohol with puking not disqualifying t all comes to the sudden death with a guy getting stripper lap dancing and they have to resist cumming eta ouse win when dgar cums with a girl dressed as a sheep on his lap lso starring lubbers hristopher conald as r tifler eghan effern as shley an etronijevic as ull ic ac as obby hristine arger as argie talia icci as aura ohnson oshana albert as ara oleman arah ower as enise ndreja unkris as tacy and ordan rentice as ock he nudity amount is very slightly increased as is the grossness of the jokes and  could guess it being rated one star out of five but  like it dequate\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_text[3])\n",
    "print(train_labels[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at all the words we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:23:15.708639Z",
     "end_time": "2023-05-17T11:23:16.260985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of unique words = 74445\n"
     ]
    }
   ],
   "source": [
    "all_words = list(chain(*[x.lower().split() for x in train_text]))\n",
    "print('total number of unique words =', len(set(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of words... of course, we could clean the data even more if we wanted to. But we won't...\n",
    "(for instance, there are probably many misspelled words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:23:28.510654Z",
     "end_time": "2023-05-17T11:23:28.963428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({'the': 114541,\n         'a': 62345,\n         'and': 61505,\n         'of': 57213,\n         'to': 53223,\n         'is': 43213,\n         'in': 35007,\n         'he': 27608,\n         'that': 26795,\n         'it': 26019,\n         'this': 24149,\n         'br': 23360,\n         'was': 18898,\n         'as': 17414,\n         'with': 17101,\n         'his': 17100,\n         'for': 16926,\n         'movie': 16521,\n         'film': 14704,\n         'but': 13720,\n         'on': 13381,\n         'are': 11623,\n         'have': 11019,\n         'not': 10945,\n         'you': 10600,\n         'be': 10515,\n         'one': 9520,\n         'an': 9091,\n         'at': 8927,\n         'by': 8753,\n         'all': 8449,\n         'who': 8073,\n         'from': 7801,\n         'or': 7762,\n         'like': 7607,\n         'its': 7407,\n         'they': 7114,\n         'so': 6866,\n         'about': 6819,\n         'her': 6795,\n         'just': 6609,\n         'has': 6570,\n         'out': 6472,\n         'some': 5698,\n         'more': 5509,\n         'very': 5492,\n         't': 5465,\n         'good': 5453,\n         'would': 4889,\n         'up': 4861,\n         'what': 4781,\n         'which': 4640,\n         'when': 4627,\n         'if': 4586,\n         'time': 4577,\n         'really': 4513,\n         'only': 4494,\n         'had': 4436,\n         'their': 4419,\n         'even': 4406,\n         'were': 4371,\n         'see': 4331,\n         'story': 4287,\n         'can': 4266,\n         'there': 4226,\n         'no': 4224,\n         'me': 4074,\n         'my': 4064,\n         'here': 4059,\n         'than': 3980,\n         'she': 3977,\n         'been': 3763,\n         'much': 3743,\n         'into': 3682,\n         'get': 3576,\n         's': 3568,\n         'bad': 3442,\n         'will': 3441,\n         'people': 3424,\n         'because': 3423,\n         'other': 3408,\n         'him': 3402,\n         'great': 3392,\n         'do': 3374,\n         'most': 3201,\n         'we': 3196,\n         'how': 3169,\n         'first': 3168,\n         'n': 3135,\n         'made': 3125,\n         'any': 3103,\n         'could': 3099,\n         'well': 3065,\n         'films': 3059,\n         'way': 3058,\n         'also': 3039,\n         'movies': 3039,\n         'them': 3036,\n         'make': 3028,\n         'e': 2992,\n         'ut': 2975,\n         'dont': 2959,\n         'too': 2920,\n         'think': 2851,\n         'characters': 2838,\n         'nd': 2789,\n         'o': 2756,\n         'ts': 2745,\n         'f': 2691,\n         'then': 2637,\n         'seen': 2633,\n         'being': 2571,\n         'character': 2521,\n         'watch': 2466,\n         'never': 2456,\n         'plot': 2454,\n         'many': 2431,\n         'ever': 2412,\n         'little': 2398,\n         'where': 2394,\n         'two': 2381,\n         'after': 2380,\n         'know': 2358,\n         'acting': 2345,\n         'best': 2286,\n         'did': 2283,\n         'does': 2282,\n         'life': 2282,\n         'love': 2282,\n         'over': 2246,\n         'better': 2202,\n         'show': 2150,\n         'off': 2147,\n         'your': 2127,\n         'say': 2102,\n         'end': 2084,\n         'scene': 2055,\n         'i': 2045,\n         'still': 1996,\n         'such': 1975,\n         'hat': 1969,\n         'scenes': 1961,\n         'should': 1944,\n         'something': 1935,\n         'go': 1931,\n         'm': 1893,\n         'through': 1882,\n         'back': 1861,\n         'these': 1851,\n         'doesnt': 1816,\n         'years': 1772,\n         'didnt': 1756,\n         'thing': 1732,\n         'actors': 1713,\n         'those': 1711,\n         'man': 1698,\n         'watching': 1689,\n         'real': 1669,\n         'though': 1660,\n         'find': 1638,\n         'funny': 1616,\n         'lot': 1608,\n         'same': 1597,\n         'old': 1589,\n         'going': 1587,\n         'why': 1580,\n         'work': 1572,\n         'few': 1571,\n         'part': 1562,\n         'hen': 1557,\n         'before': 1546,\n         'look': 1542,\n         'makes': 1535,\n         'while': 1506,\n         'actually': 1492,\n         'nothing': 1481,\n         'cant': 1477,\n         'another': 1466,\n         'want': 1450,\n         'quite': 1435,\n         'cast': 1431,\n         'got': 1409,\n         'again': 1408,\n         'fact': 1388,\n         'every': 1384,\n         'pretty': 1374,\n         'seems': 1371,\n         'own': 1368,\n         'around': 1358,\n         'things': 1352,\n         'take': 1350,\n         'down': 1343,\n         'enough': 1328,\n         'now': 1319,\n         'director': 1315,\n         'always': 1306,\n         'thought': 1302,\n         'young': 1297,\n         'gets': 1289,\n         'us': 1285,\n         'right': 1282,\n         'long': 1282,\n         'between': 1282,\n         'am': 1279,\n         've': 1271,\n         'isnt': 1271,\n         'give': 1265,\n         'without': 1253,\n         'least': 1252,\n         'series': 1248,\n         'saw': 1246,\n         'hey': 1229,\n         'action': 1218,\n         'horror': 1213,\n         'role': 1213,\n         'interesting': 1208,\n         'both': 1205,\n         'may': 1202,\n         'family': 1191,\n         'whole': 1191,\n         'must': 1191,\n         'script': 1184,\n         'original': 1183,\n         'point': 1175,\n         'bit': 1170,\n         'times': 1167,\n         'r': 1165,\n         'come': 1165,\n         'll': 1161,\n         'new': 1158,\n         'big': 1157,\n         'almost': 1154,\n         'done': 1153,\n         'y': 1151,\n         'minutes': 1151,\n         'world': 1150,\n         'comedy': 1121,\n         'might': 1120,\n         'far': 1117,\n         'probably': 1116,\n         'ou': 1111,\n         'anything': 1110,\n         'thats': 1106,\n         'feel': 1103,\n         'guy': 1095,\n         'performance': 1086,\n         'kind': 1083,\n         'hes': 1076,\n         'music': 1073,\n         'ow': 1058,\n         'last': 1055,\n         'our': 1052,\n         'since': 1046,\n         'rather': 1039,\n         'away': 1022,\n         'found': 1017,\n         'making': 1011,\n         'fun': 999,\n         'played': 985,\n         'girl': 958,\n         'woman': 954,\n         'trying': 954,\n         'believe': 954,\n         'worst': 943,\n         'goes': 942,\n         'comes': 938,\n         'course': 938,\n         'looks': 925,\n         'wasnt': 923,\n         'each': 917,\n         'hard': 917,\n         'different': 911,\n         'put': 909,\n         'ne': 905,\n         'seem': 902,\n         'reason': 901,\n         'shows': 896,\n         'place': 894,\n         'ending': 889,\n         'plays': 889,\n         'money': 889,\n         'especially': 884,\n         'sense': 883,\n         'ot': 882,\n         'looking': 881,\n         'main': 878,\n         'set': 877,\n         'yet': 872,\n         'job': 871,\n         'ohn': 869,\n         'sure': 868,\n         'anyone': 866,\n         'someone': 857,\n         'having': 856,\n         'day': 853,\n         'screen': 850,\n         'worth': 849,\n         'book': 849,\n         'true': 849,\n         'takes': 844,\n         'merican': 839,\n         'said': 833,\n         'beautiful': 832,\n         'watched': 825,\n         'actor': 825,\n         'together': 820,\n         'version': 814,\n         'himself': 812,\n         'seeing': 800,\n         'audience': 798,\n         'left': 797,\n         'play': 794,\n         'effects': 790,\n         'everything': 787,\n         'excellent': 782,\n         'shot': 781,\n         'three': 779,\n         'later': 776,\n         'wife': 774,\n         'owever': 772,\n         'else': 769,\n         'fan': 767,\n         'nice': 765,\n         'less': 763,\n         'once': 757,\n         'youre': 756,\n         'simply': 753,\n         'special': 748,\n         'during': 745,\n         'used': 742,\n         'theres': 739,\n         'read': 737,\n         'everyone': 733,\n         'either': 733,\n         'idea': 730,\n         'year': 723,\n         'help': 719,\n         'completely': 717,\n         'ell': 713,\n         'kids': 711,\n         'short': 711,\n         'rest': 706,\n         'until': 705,\n         'need': 701,\n         'second': 695,\n         'ollywood': 687,\n         'mind': 687,\n         'performances': 686,\n         'use': 684,\n         'try': 683,\n         'father': 682,\n         'half': 680,\n         'given': 674,\n         'came': 671,\n         'poor': 667,\n         'es': 663,\n         'friends': 659,\n         'ome': 658,\n         'truly': 656,\n         'women': 656,\n         'recommend': 655,\n         'line': 648,\n         'start': 647,\n         'ven': 647,\n         'boring': 645,\n         'home': 644,\n         'enjoy': 644,\n         'wrong': 641,\n         'classic': 639,\n         'tell': 637,\n         'production': 635,\n         'understand': 634,\n         'remember': 634,\n         'fter': 633,\n         'moments': 633,\n         'instead': 631,\n         'become': 629,\n         'along': 629,\n         'couple': 629,\n         'wonderful': 629,\n         'gives': 627,\n         'however': 627,\n         'mean': 626,\n         'doing': 625,\n         'lack': 624,\n         'playing': 624,\n         'd': 622,\n         'awful': 616,\n         'others': 616,\n         'death': 615,\n         'high': 615,\n         'although': 613,\n         'men': 611,\n         'stupid': 611,\n         'maybe': 606,\n         'often': 604,\n         'next': 604,\n         'camera': 604,\n         'small': 602,\n         'live': 601,\n         'supposed': 599,\n         'dialogue': 598,\n         'perfect': 598,\n         'lines': 596,\n         'full': 595,\n         'entire': 595,\n         'felt': 595,\n         'name': 595,\n         'night': 592,\n         'video': 591,\n         'ones': 590,\n         'piece': 589,\n         'ill': 589,\n         'getting': 588,\n         'problem': 587,\n         'episode': 586,\n         'early': 585,\n         'ay': 585,\n         'itself': 585,\n         'budget': 581,\n         'top': 580,\n         'face': 577,\n         'hile': 577,\n         'stars': 577,\n         'title': 570,\n         'terrible': 570,\n         'against': 568,\n         'becomes': 567,\n         'went': 564,\n         'keep': 564,\n         'sex': 564,\n         'house': 564,\n         'head': 563,\n         'picture': 561,\n         'couldnt': 560,\n         'liked': 559,\n         'person': 557,\n         'let': 551,\n         'case': 550,\n         'waste': 548,\n         'ew': 547,\n         'wants': 547,\n         'human': 545,\n         'sort': 543,\n         'lives': 543,\n         'care': 542,\n         'several': 541,\n         'style': 540,\n         'definitely': 540,\n         'heres': 540,\n         'entertaining': 538,\n         'school': 537,\n         'absolutely': 537,\n         'black': 533,\n         'beginning': 533,\n         'lso': 532,\n         'fans': 531,\n         'example': 529,\n         'able': 528,\n         'worse': 526,\n         'children': 526,\n         'loved': 525,\n         'friend': 524,\n         'lead': 524,\n         'wanted': 524,\n         'certainly': 524,\n         'seemed': 522,\n         'hope': 521,\n         'war': 519,\n         'hy': 518,\n         'low': 517,\n         'art': 516,\n         'guys': 515,\n         'wont': 514,\n         'im': 513,\n         'age': 511,\n         'written': 510,\n         'side': 506,\n         'already': 505,\n         'lost': 504,\n         'ichael': 501,\n         'star': 500,\n         'based': 500,\n         'hats': 500,\n         'enjoyed': 499,\n         'turns': 499,\n         'totally': 498,\n         'works': 497,\n         'writing': 496,\n         'mother': 496,\n         'fine': 495,\n         'quality': 494,\n         'starts': 493,\n         'laugh': 493,\n         'stories': 492,\n         'turn': 490,\n         'drama': 490,\n         'cinema': 489,\n         'humor': 488,\n         'direction': 488,\n         'under': 487,\n         'past': 487,\n         'sound': 486,\n         'favorite': 485,\n         'tries': 484,\n         'evil': 483,\n         'son': 482,\n         'gave': 480,\n         'ith': 479,\n         'ee': 479,\n         'parts': 477,\n         'throughout': 476,\n         'finally': 475,\n         'guess': 473,\n         'called': 472,\n         'ames': 472,\n         'dead': 472,\n         'town': 472,\n         'girls': 471,\n         'flick': 467,\n         'boy': 465,\n         'ack': 464,\n         'ust': 464,\n         'themselves': 463,\n         'matter': 460,\n         'et': 460,\n         'brilliant': 460,\n         'killed': 460,\n         'en': 458,\n         'h': 458,\n         'amazing': 458,\n         'final': 457,\n         'perhaps': 453,\n         'behind': 452,\n         'took': 451,\n         'expect': 449,\n         'child': 449,\n         'hour': 448,\n         'act': 446,\n         'kill': 445,\n         'says': 444,\n         'shes': 444,\n         'horrible': 442,\n         'genre': 441,\n         'actress': 441,\n         'ont': 440,\n         'feeling': 439,\n         'myself': 439,\n         'whose': 439,\n         'involved': 438,\n         'roles': 437,\n         'viewer': 436,\n         'killer': 436,\n         'run': 436,\n         'attempt': 435,\n         'including': 435,\n         'theyre': 435,\n         'decent': 434,\n         'cannot': 431,\n         'car': 431,\n         'game': 429,\n         'dark': 428,\n         'etc': 428,\n         'hand': 428,\n         'late': 428,\n         'moment': 428,\n         'avid': 424,\n         'fight': 423,\n         'type': 423,\n         'est': 421,\n         'extremely': 421,\n         'heard': 420,\n         'thinking': 419,\n         'heart': 418,\n         'group': 416,\n         'except': 416,\n         'wouldnt': 415,\n         'stuff': 415,\n         'leave': 414,\n         'happened': 410,\n         'career': 410,\n         'history': 409,\n         'obvious': 408,\n         'shown': 407,\n         'ago': 406,\n         'sometimes': 406,\n         'youll': 405,\n         'close': 403,\n         'itbr': 402,\n         'soon': 402,\n         'wonder': 401,\n         'kid': 401,\n         'somewhat': 401,\n         'irst': 400,\n         'chance': 400,\n         'days': 399,\n         'obviously': 399,\n         'happens': 399,\n         'score': 399,\n         'told': 397,\n         'strong': 396,\n         'coming': 396,\n         'particularly': 396,\n         'happen': 396,\n         'eyes': 394,\n         'directed': 393,\n         'violence': 393,\n         'ike': 392,\n         'experience': 392,\n         'interest': 391,\n         'novel': 391,\n         'usual': 389,\n         'ends': 388,\n         'saying': 388,\n         'nglish': 384,\n         'known': 384,\n         'ing': 383,\n         'released': 383,\n         'crap': 381,\n         'complete': 380,\n         'alone': 380,\n         'daughter': 379,\n         'looked': 379,\n         'brother': 378,\n         'exactly': 377,\n         'taken': 376,\n         'number': 376,\n         'song': 376,\n         'hit': 375,\n         'voice': 375,\n         'arent': 375,\n         'sad': 375,\n         'across': 374,\n         'slow': 374,\n         'ost': 374,\n         'musical': 374,\n         'white': 374,\n         'lthough': 372,\n         'serious': 372,\n         'finds': 371,\n         'husband': 371,\n         'relationship': 371,\n         'knows': 371,\n         'obert': 371,\n         'important': 370,\n         'talking': 370,\n         'mostly': 370,\n         'disappointed': 369,\n         'jokes': 368,\n         'living': 368,\n         'murder': 368,\n         'aul': 368,\n         'ane': 368,\n         'hours': 367,\n         'stop': 367,\n         'simple': 367,\n         'huge': 366,\n         'highly': 365,\n         'taking': 365,\n         'wish': 363,\n         'opening': 363,\n         'shots': 362,\n         'started': 361,\n         'light': 359,\n         'cinematography': 359,\n         'female': 358,\n         'hilarious': 356,\n         'eorge': 356,\n         'ad': 355,\n         'ridiculous': 355,\n         'change': 355,\n         'running': 354,\n         'annoying': 354,\n         'order': 354,\n         'aybe': 353,\n         'turned': 352,\n         'local': 352,\n         'words': 352,\n         'major': 352,\n         'ary': 352,\n         'tells': 351,\n         'episodes': 351,\n         'police': 351,\n         'attention': 351,\n         'possible': 350,\n         'ork': 349,\n         'despite': 349,\n         'happy': 349,\n         'ritish': 349,\n         'usually': 348,\n         'easily': 348,\n         'scary': 346,\n         'cheap': 346,\n         'om': 345,\n         'viewers': 345,\n         'oe': 344,\n         'save': 344,\n         'directors': 344,\n         'ichard': 344,\n         'opinion': 343,\n         'havent': 343,\n         'future': 342,\n         'reality': 342,\n         'yourself': 342,\n         'strange': 341,\n         'events': 341,\n         'due': 340,\n         'songs': 340,\n         'feels': 339,\n         'falls': 339,\n         'today': 339,\n         'level': 338,\n         'clearly': 338,\n         'single': 338,\n         'call': 335,\n         'mention': 332,\n         'bring': 332,\n         'filmbr': 332,\n         'above': 330,\n         'word': 330,\n         'silly': 330,\n         'body': 330,\n         'knew': 329,\n         'view': 328,\n         'cool': 328,\n         'rom': 328,\n         'appears': 327,\n         'predictable': 327,\n         'nfortunately': 327,\n         'moviebr': 326,\n         'room': 325,\n         'modern': 325,\n         'review': 324,\n         'ood': 323,\n         'problems': 323,\n         'storyline': 323,\n         'arry': 322,\n         'hero': 322,\n         'er': 322,\n         'tried': 321,\n         'moving': 320,\n         'hell': 320,\n         'four': 319,\n         'talent': 319,\n         'supporting': 318,\n         'herself': 318,\n         'documentary': 318,\n         'giving': 318,\n         'buy': 317,\n         'sequence': 317,\n         'within': 316,\n         'cut': 316,\n         'od': 315,\n         'points': 315,\n         'television': 315,\n         'gore': 315,\n         'blood': 315,\n         'soundtrack': 315,\n         'similar': 313,\n         'ho': 313,\n         'thriller': 313,\n         'entertainment': 312,\n         'sets': 311,\n         'needs': 311,\n         'ark': 310,\n         'dull': 310,\n         'message': 309,\n         'country': 309,\n         'form': 309,\n         'typical': 309,\n         'ways': 308,\n         'greatest': 307,\n         'upon': 306,\n         'ar': 306,\n         'none': 305,\n         'comic': 304,\n         'writer': 304,\n         'named': 304,\n         'yes': 303,\n         'nearly': 303,\n         'dialog': 303,\n         'realistic': 302,\n         'eter': 302,\n         'famous': 302,\n         'animation': 301,\n         'ean': 301,\n         'surprised': 300,\n         'elements': 300,\n         'comments': 299,\n         'seriously': 299,\n         'theme': 299,\n         'ight': 299,\n         'actual': 297,\n         'ere': 297,\n         'red': 297,\n         'doubt': 296,\n         'weak': 295,\n         'clear': 295,\n         'beyond': 294,\n         'overall': 294,\n         'ong': 294,\n         'theater': 293,\n         'parents': 293,\n         'brought': 293,\n         'working': 293,\n         'question': 292,\n         'hear': 292,\n         'release': 292,\n         'easy': 291,\n         'bunch': 290,\n         'fall': 290,\n         'enjoyable': 290,\n         'scar': 289,\n         'sexual': 289,\n         'talk': 289,\n         'rent': 289,\n         'th': 289,\n         'romantic': 289,\n         'showing': 288,\n         'kept': 287,\n         'till': 287,\n         'team': 286,\n         'nature': 285,\n         'using': 285,\n         'deal': 285,\n         'middle': 285,\n         'writers': 284,\n         'difficult': 284,\n         'believable': 284,\n         'ed': 283,\n         'decided': 283,\n         'material': 282,\n         'stay': 282,\n         'leaves': 282,\n         'atmosphere': 282,\n         'feature': 282,\n         'power': 282,\n         'effort': 281,\n         'became': 281,\n         'fantastic': 280,\n         'hate': 280,\n         'expected': 280,\n         'viewing': 280,\n         'near': 280,\n         'keeps': 279,\n         'sit': 279,\n         'leads': 279,\n         'means': 278,\n         'ten': 277,\n         'earlier': 277,\n         'basically': 276,\n         'filmed': 276,\n         'appear': 276,\n         'class': 276,\n         'stage': 276,\n         'certain': 275,\n         'figure': 275,\n         'sister': 275,\n         'learn': 275,\n         'premise': 275,\n         'brings': 275,\n         'lots': 275,\n         'particular': 275,\n         'lame': 275,\n         'dramatic': 274,\n         'five': 273,\n         'emotional': 273,\n         'hese': 272,\n         'add': 272,\n         'sequel': 272,\n         'meet': 272,\n         'realize': 270,\n         'credits': 270,\n         'surprise': 270,\n         'rench': 270,\n         'average': 269,\n         'rating': 268,\n         'whether': 267,\n         'straight': 267,\n         'sequences': 267,\n         'editing': 267,\n         'begins': 267,\n         'apanese': 267,\n         'lets': 266,\n         'nor': 266,\n         'die': 266,\n         'subject': 266,\n         'truth': 266,\n         'write': 266,\n         'stand': 265,\n         'city': 264,\n         'among': 262,\n         'tale': 262,\n         'needed': 262,\n         'dance': 262,\n         'apparently': 262,\n         'follow': 261,\n         'meets': 260,\n         'male': 260,\n         'minute': 260,\n         'pace': 259,\n         'memorable': 259,\n         'fairly': 258,\n         'comment': 258,\n         'ask': 258,\n         'footage': 257,\n         'youve': 257,\n         'verall': 256,\n         'begin': 256,\n         'whos': 255,\n         'somehow': 255,\n         'move': 255,\n         'b': 255,\n         'older': 255,\n         'reading': 254,\n         'features': 253,\n         'interested': 253,\n         'poorly': 253,\n         'nother': 253,\n         'period': 253,\n         'meant': 252,\n         'personal': 251,\n         'superb': 251,\n         'expecting': 251,\n         'various': 251,\n         'unique': 251,\n         'total': 250,\n         'development': 250,\n         'eye': 250,\n         'isney': 250,\n         'forced': 250,\n         'please': 250,\n         'deserves': 249,\n         'weird': 248,\n         'tone': 248,\n         'potential': 248,\n         'situation': 247,\n         'gone': 247,\n         'forward': 247,\n         'create': 247,\n         'reasons': 247,\n         'imagine': 247,\n         'reviews': 247,\n         'incredibly': 246,\n         'towards': 246,\n         'killing': 246,\n         'directing': 246,\n         'screenplay': 246,\n         'result': 245,\n         'missing': 245,\n         'ead': 244,\n         'erry': 244,\n         'girlfriend': 243,\n         'open': 243,\n         'filmmakers': 242,\n         'powerful': 242,\n         'crime': 242,\n         'ideas': 242,\n         'ick': 241,\n         'forget': 241,\n         'reat': 241,\n         'casting': 241,\n         'fast': 240,\n         'quickly': 240,\n         'effect': 240,\n         'creepy': 239,\n         'mess': 239,\n         'admit': 239,\n         'badly': 239,\n         'previous': 238,\n         'worked': 238,\n         'hristmas': 238,\n         'merica': 238,\n         'boys': 237,\n         'shame': 237,\n         'possibly': 237,\n         'sorry': 236,\n         'hands': 236,\n         'whats': 235,\n         'wait': 235,\n         'fails': 234,\n         'perfectly': 234,\n         'romance': 233,\n         'outside': 233,\n         'inside': 233,\n         'slightly': 232,\n         'odd': 232,\n         'general': 231,\n         'ouse': 231,\n         'deep': 231,\n         'beauty': 231,\n         'suspense': 231,\n         'plenty': 230,\n         'members': 230,\n         'cute': 230,\n         'illiam': 230,\n         'mentioned': 230,\n         'third': 229,\n         'leading': 229,\n         'miss': 229,\n         ...})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:23:40.091330Z",
     "end_time": "2023-05-17T11:23:40.115798Z"
    }
   },
   "outputs": [],
   "source": [
    "n_words = 25000   # let's make a model that only understand 25000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:23:46.475292Z",
     "end_time": "2023-05-17T11:23:49.649214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<UNK>', 'the', 'a', 'and', 'of'] ... ['bumblers', 'meki', 'lowp', 'stonecold', 'vying']\n"
     ]
    }
   ],
   "source": [
    "words, count = np.unique(all_words, return_counts=True)\n",
    "idxs = np.argsort(count)[-n_words:]\n",
    "vocab = ['<UNK>'] + list(words[idxs][::-1])\n",
    "print(vocab[:5], '...', vocab[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very surprisingly, the most commonly used word is _the_. The 25000th most used word is _chimneys_. We have added a special word `<UNK>` which we will used to mark words outside our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now turn a sentence into a sequence of integers that correspond to the position in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:24:18.742870Z",
     "end_time": "2023-05-17T11:24:18.775048Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_d = {vocab[i]: i for i in range(len(vocab))}  # for quick look-up\n",
    "def sentence_to_integer_sequence(s):\n",
    "    return torch.tensor([vocab_d[x] if x in vocab_d else 0 for x in s.split()], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:24:22.479723Z",
     "end_time": "2023-05-17T11:24:22.500108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([138,  56, 436,   1,  18,   0])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_integer_sequence(\"i really liked the movie xenopus51\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representing high dimensional spaces with a simpler (learnable) embedding:\n",
    "\n",
    "We are now representing words in a \"25000\"-dimensional space: we have a unique integer for each word we can represent. To reduce this complexity, we instead intend to represent each word as a 50-dimensional real vector. Pytorch to the rescue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:24:37.150101Z",
     "end_time": "2023-05-17T11:24:37.204734Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(vocab), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Embedding` assigns are random, **but trainable** vector to each word. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:24:40.709999Z",
     "end_time": "2023-05-17T11:24:40.727005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3830,  1.0040,  0.8199, -0.1486, -0.4464,  0.8554,  0.9904, -0.2797,\n",
      "         -1.4488,  0.3964,  0.8499, -0.1861, -0.5887, -0.0169, -0.4246,  0.4436,\n",
      "          0.3712,  1.0042,  0.2687, -0.5322, -0.8029,  1.0065, -0.6357, -1.4029,\n",
      "          1.3472,  1.2819,  0.2463, -0.1311, -1.1287, -0.4313,  2.2918,  1.2813,\n",
      "          0.7973,  1.4662, -0.6910,  1.6100, -2.2428,  1.0082,  0.7776, -1.0881,\n",
      "          0.7889, -1.8485,  0.0176, -1.8129, -0.7277,  0.0706, -1.1938, -0.3545,\n",
      "         -0.0350,  0.2063]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding(sentence_to_integer_sequence(\"movie\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The special `<UNK>` word, which signifies unknown we can choose to zero out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:24:45.092266Z",
     "end_time": "2023-05-17T11:24:45.107414Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding.weight.data[0, :] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how we represent a sentence then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:24:48.914830Z",
     "end_time": "2023-05-17T11:24:48.927383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 50])\n",
      "tensor([[ 1.8757e-01,  1.1478e+00, -4.0609e-01,  7.9164e-01, -1.3440e+00,\n",
      "          1.3291e+00,  2.7436e-01,  1.4376e+00,  6.3498e-01, -5.0623e-01,\n",
      "         -8.0561e-01, -7.7071e-01,  2.1853e+00, -1.0426e+00,  3.4722e-01,\n",
      "          5.4782e-02,  7.2509e-01,  1.1170e+00,  8.1339e-01, -8.6629e-01,\n",
      "         -3.0913e-01,  1.5571e+00, -1.4386e-01, -1.0269e+00,  6.0549e-01,\n",
      "         -7.4567e-01, -1.0668e+00, -8.5426e-01, -6.5357e-01,  1.5315e+00,\n",
      "          6.4485e-01, -3.7481e-01, -4.5751e-01, -2.4071e-01, -2.0029e+00,\n",
      "          2.5380e+00, -3.6887e-01, -1.7410e+00, -1.0699e+00, -5.6962e-01,\n",
      "         -2.9811e-01,  3.5479e-01, -1.4555e+00, -8.3069e-01,  1.0354e+00,\n",
      "         -1.2415e-01, -6.7981e-01, -2.9255e-01, -5.1272e-01, -1.0905e+00],\n",
      "        [-3.1909e-01, -3.5195e-01, -7.2693e-02,  1.3847e-01,  2.6423e-03,\n",
      "          1.3508e+00, -2.1623e+00,  1.5713e+00, -6.1308e-01,  2.5192e-01,\n",
      "         -8.1770e-01,  1.4639e+00,  2.3467e-01, -9.4482e-01, -4.9316e-01,\n",
      "          5.9213e-01,  3.3163e-01,  1.5318e-01,  1.9219e-01, -7.3506e-01,\n",
      "          7.8678e-01,  4.7498e-01,  1.6877e+00,  7.3288e-01,  9.5161e-01,\n",
      "          8.3006e-02,  6.5277e-01,  7.1151e-01, -5.7186e-01,  5.9646e-01,\n",
      "         -7.6607e-01,  1.3025e-01, -1.0003e+00,  2.4022e-01,  1.6796e+00,\n",
      "         -5.1254e-01,  1.2603e-01,  4.9629e-01, -5.3292e-01, -1.2922e+00,\n",
      "         -1.2985e+00,  2.4812e-01,  6.9229e-02,  2.2215e-02,  2.1799e-01,\n",
      "         -6.3463e-02,  1.1258e+00,  2.1117e+00,  7.0023e-01,  1.0868e-01],\n",
      "        [-4.2289e-01,  9.6234e-01,  9.1922e-01,  1.2926e+00, -1.1053e+00,\n",
      "         -5.7992e-01, -3.9119e-01, -1.4944e+00,  6.4975e-01, -4.5172e-01,\n",
      "          9.3458e-01, -2.2931e+00,  1.0994e-01, -2.6698e-01, -7.2128e-01,\n",
      "          1.4063e+00, -1.6832e+00,  1.7825e+00, -5.3665e-01, -2.9091e-01,\n",
      "          8.0093e-02,  7.2130e-02, -1.5277e+00,  4.4676e-01,  8.0593e-02,\n",
      "         -4.8187e-02, -1.0618e+00,  2.0343e-01, -3.2531e-01,  6.4194e-01,\n",
      "         -6.6324e-01,  6.1286e-02, -3.9121e-01, -2.2880e-01,  4.5776e-01,\n",
      "          4.2952e-01, -1.8224e+00, -5.3555e-01, -2.4725e+00, -1.6949e-02,\n",
      "         -6.1392e-01,  1.3500e+00, -1.2765e+00,  2.4640e-01,  1.0711e+00,\n",
      "         -2.7530e-01,  1.4807e-01, -3.3933e-01,  5.9389e-01,  1.4786e+00],\n",
      "        [ 2.5373e-01, -1.4676e-01,  6.4518e-01, -1.2660e-01,  1.6545e+00,\n",
      "          1.2424e-02,  1.1450e-01,  3.3346e-01,  1.8486e-01,  3.9502e-01,\n",
      "          2.6305e+00, -1.0033e+00, -2.8384e-01,  1.4531e-02,  4.2236e-01,\n",
      "         -1.5109e-01,  2.9848e-01,  1.6829e+00,  2.2297e-01, -3.6298e-01,\n",
      "          1.8417e+00,  1.5893e+00, -4.6658e-01,  1.0054e+00,  2.8915e-01,\n",
      "         -9.4090e-01, -9.2444e-01, -6.9180e-02, -3.0030e-01, -7.9932e-01,\n",
      "          7.6631e-02, -2.2632e-01, -8.3445e-01,  1.4750e+00, -8.6651e-01,\n",
      "         -9.3203e-02,  9.6275e-01,  1.6704e+00, -1.1812e+00,  1.9297e-05,\n",
      "         -3.7652e-03, -6.0418e-01, -1.0884e-01, -1.0822e+00,  6.9586e-02,\n",
      "          7.5749e-01,  1.8930e+00, -7.3617e-02,  2.1361e+00, -6.2719e-01],\n",
      "        [ 3.8298e-01,  1.0040e+00,  8.1986e-01, -1.4858e-01, -4.4641e-01,\n",
      "          8.5540e-01,  9.9043e-01, -2.7969e-01, -1.4488e+00,  3.9640e-01,\n",
      "          8.4994e-01, -1.8614e-01, -5.8872e-01, -1.6880e-02, -4.2455e-01,\n",
      "          4.4362e-01,  3.7118e-01,  1.0042e+00,  2.6873e-01, -5.3220e-01,\n",
      "         -8.0293e-01,  1.0065e+00, -6.3573e-01, -1.4029e+00,  1.3472e+00,\n",
      "          1.2819e+00,  2.4625e-01, -1.3105e-01, -1.1287e+00, -4.3128e-01,\n",
      "          2.2918e+00,  1.2813e+00,  7.9730e-01,  1.4662e+00, -6.9101e-01,\n",
      "          1.6100e+00, -2.2428e+00,  1.0082e+00,  7.7758e-01, -1.0881e+00,\n",
      "          7.8894e-01, -1.8485e+00,  1.7581e-02, -1.8129e+00, -7.2771e-01,\n",
      "          7.0563e-02, -1.1938e+00, -3.5449e-01, -3.5004e-02,  2.0633e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding(sentence_to_integer_sequence(\"i really liked the movie xenopus51\")).shape)\n",
    "print(embedding(sentence_to_integer_sequence(\"i really liked the movie xenopus51\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, the sentence bascially becomes a 6x50 pixel image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use `nn.LSTM` after an embedding to define a neural network for sentenes.\n",
    "\n",
    "This network will have a _lot_ of parameters. For each word, 50 parameters needs to be trained, and then comes the LSTM on top of that.\n",
    "This is the reason that _transfer learning_ is so important in natural language processing (NLP).\n",
    "\n",
    "Perhaps the simplest form of transfer learning is to use a pretrained embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:25:22.487760Z",
     "end_time": "2023-05-17T11:25:24.224855Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('glove.6B.50d.pkl', 'rb') as f:\n",
    "    glove = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:25:25.991687Z",
     "end_time": "2023-05-17T11:25:26.037150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.30824 ,  0.17223 , -0.23339 ,  0.023105,  0.28522 ,  0.23076 ,\n       -0.41048 , -1.0035  , -0.2072  ,  1.4327  , -0.80684 ,  0.68954 ,\n       -0.43648 ,  1.1069  ,  1.6107  , -0.31966 ,  0.47744 ,  0.79395 ,\n       -0.84374 ,  0.064509,  0.90251 ,  0.78609 ,  0.29699 ,  0.76057 ,\n        0.433   , -1.5032  , -1.6423  ,  0.30256 ,  0.30771 , -0.87057 ,\n        2.4782  , -0.025852,  0.5013  , -0.38593 , -0.15633 ,  0.45522 ,\n        0.04901 , -0.42599 , -0.86402 , -1.3076  , -0.29576 ,  1.209   ,\n       -0.3127  , -0.72462 , -0.80801 ,  0.082667,  0.26738 , -0.98177 ,\n       -0.32147 ,  0.99823 ])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['movie']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These pretrained word embeddings will have a good structure to them. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:26:04.154680Z",
     "end_time": "2023-05-17T11:26:04.169131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance b/w queen and prince = 3.3926491063677284\n",
      "Distance b/w movie and prince = 6.450784821820618\n"
     ]
    }
   ],
   "source": [
    "print('Distance b/w queen and prince =', np.linalg.norm(glove['queen'] - glove['prince']))\n",
    "print('Distance b/w movie and prince =', np.linalg.norm(glove['movie'] - glove['prince']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even sometimes get away with doing algebra with these vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:26:08.862604Z",
     "end_time": "2023-05-17T11:26:08.881075Z"
    }
   },
   "outputs": [],
   "source": [
    "queenlike = glove['king'] - glove['man'] + glove['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:26:32.280225Z",
     "end_time": "2023-05-17T11:26:32.326595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance b/w queen and algebraic queen = 2.8391206432941996\n",
      "Distance b/w queen and king = 3.4777562289742345\n"
     ]
    }
   ],
   "source": [
    "print('Distance b/w queen and algebraic queen =', np.linalg.norm(glove['queen'] - queenlike))\n",
    "print('Distance b/w queen and king =', np.linalg.norm(glove['queen'] - glove['king']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill out our embedding layer using these pretrained vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:26:41.957082Z",
     "end_time": "2023-05-17T11:26:42.556522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the words in the vocab, 74.0 % were updated using Glove vectors\n",
      "Examples of words not found : ['owever', 'ichael', 'itbr', 'nglish', 'eorge', 'aybe']\n"
     ]
    }
   ],
   "source": [
    "filled = 0\n",
    "not_found = []\n",
    "for i, w in enumerate(vocab):\n",
    "    if w in glove:\n",
    "        embedding.weight.data[i, :] = torch.tensor(glove[w], dtype=torch.float)\n",
    "        filled += 1\n",
    "    else:\n",
    "        not_found.append(w)\n",
    "print(f'Of the words in the vocab, {100 * filled / (len(vocab) - 1)} % were updated using Glove vectors')\n",
    "print('Examples of words not found :', not_found[1:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the words we do not find are due to misspellings. You now have three choices before you continue with the exercise:\n",
    "\n",
    "(1) Use _hunspell_ or similar to fix misspelled words\n",
    "\n",
    "(2) Change the vocabulary (`vocab`) to be based on words that are both frequent in the text and have glove vectors\n",
    "\n",
    "(3) Ignore the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with Glove vectors we do not need to train the embedding. We can consider it fixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:29:23.176423Z",
     "end_time": "2023-05-17T11:29:23.186509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Embedding(25001, 50)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a neural network using `nn.LSTM` layers to classify the IMDB reviews.\n",
    "\n",
    "Ideas:\n",
    "\n",
    " - The sentences can be quite long, so you might want to limit them to, say, maximum 100 words\n",
    " - `nn.LSTM` can take batched input, but be careful with `batch_first=True/False`.\n",
    " - If batched input are used, they normally have to be equal in size. You can put sentences to always be 100 words long, by adding `<UNK>` words to short sentences.\n",
    " - Alternatively, `nn.LSTM` can also accept batched, variable-length input using `torch.nn.utils.rnn.pack_sequence`.\n",
    " - (a finaly, albeit slow, alternative is to simple run the LSTM on un-batched input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often in NLP, a lot of unlabelled text is available. We can use this to pretrain the model, before fine-tuning to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:29:28.280330Z",
     "end_time": "2023-05-17T11:29:28.508155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "with open('unlabelled.json') as f:\n",
    "    text = json.load(f)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to pretrain is to train a _language model_. This is a model that tries to predict the next word in a sentence. For instance, given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:29:33.793603Z",
     "end_time": "2023-05-17T11:29:33.809535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'This is still a pretty bad and silly simplistic typical slasher but when being compared to the previous sequel \"Slumber Party Massacre II\" this movie is a step'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(text[2].split()[:28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can you guess the next word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a language model, we consider the above the input, and the output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T11:29:38.212000Z",
     "end_time": "2023-05-17T11:29:38.225344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'up'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[2].split()[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input we encode using the embedding, while the output is a probability map over words.\n",
    "In other words, the last layer is something like `nn.Linear(..., len(vocab))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import lightning as L"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-17T11:33:49.609039Z",
     "end_time": "2023-05-17T11:33:51.740503Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, hidden_size=200, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.Transformer(nhead=8, num_decoder_layers=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a language model.\n",
    "\n",
    "After you have trained the model, try to make to complete sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "### Exercise 3 (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discard the last layer of the now trained language model and use it to train on the original IMDB-problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

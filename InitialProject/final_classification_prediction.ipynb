{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-19T23:04:06.215401Z",
     "end_time": "2023-05-19T23:04:15.740432Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 23:04:13.711124: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 23:04:14.848673: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-19 23:04:14.848821: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-19 23:04:14.848831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import iminuit as iminuit\n",
    "import numba\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from model import LightningFullyConnected, BigLightningModel, FullyConnectedModel\n",
    "from verstack import LGBMTuner\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataset import ParticleDataset\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1\n",
      "[LightGBM] [Warning] num_threads is set=6, n_jobs=-1 will be ignored. Current value: num_threads=6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9318657440733003, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9318657440733003\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.289813314023732, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.289813314023732\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8607121643836452, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8607121643836452\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n",
      "(162500,)\n",
      "(162500,)\n",
      "(162500,)\n"
     ]
    }
   ],
   "source": [
    "APP_ML_PATH = \"/home/amh/Documents/Coding/GitHub/AppliedML2023\"\n",
    "\n",
    "# NN hyperparameters\n",
    "hidden_channels = 20\n",
    "decode_channels = 6\n",
    "hidden_layers = 5\n",
    "p_dropout = 0.2\n",
    "lr = 0.0003\n",
    "activation=nn.LeakyReLU\n",
    "final_activation=nn.Sigmoid\n",
    "batch_size=2500\n",
    "optimizer=optim.AdamW\n",
    "scheduler=optim.lr_scheduler.CosineAnnealingLR\n",
    "loss_fn=F.binary_cross_entropy\n",
    "in_channels=15\n",
    "out_channels=1\n",
    "use_wandb=True\n",
    "\n",
    "#LGBM hyperparameters\n",
    "lgbm_hyper = {'task': 'train', 'learning_rate': 0.04, 'num_leaves': 166, 'feature_fraction': 0.8607121643836452, 'bagging_fraction': 0.9318657440733003, 'bagging_freq': 1, 'max_depth': -1, 'verbosity': -1, 'lambda_l1': 1, 'lambda_l2': 0.0, 'min_split_gain': 0.0, 'zero_as_missing': False, 'max_bin': 255, 'min_data_in_bin': 3, 'random_state': 42, 'device_type': 'cpu', 'num_classes': 1, 'objective': 'binary', 'metric': 'binary_logloss', 'num_threads': 6, 'min_sum_hessian_in_leaf': 0.289813314023732, 'num_iterations': 313}\n",
    "\n",
    "# Catboost hyperparameters\n",
    "best_params = {'iterations': 1951, 'learning_rate': 0.04882009276925184, 'depth': 6, 'l2_leaf_reg': 2.0575408311391996, 'bootstrap_type': 'Poisson', 'random_strength': 1.371249448161803, 'bagging_temperature': 6.372416913357105, 'od_type': 'IncToDec', 'od_wait': 40}\n",
    "\n",
    "# NN classifier - Fully trained\n",
    "model1: LightningFullyConnected = LightningFullyConnected.load_from_checkpoint(\"../data/initial/nn_clf_final.ckpt\")\n",
    "model1.to(\"cpu\")\n",
    "\n",
    "# LGBM classifier\n",
    "lgbm = LGBMClassifier(**lgbm_hyper)\n",
    "\n",
    "# Catboost classifier\n",
    "cat_clf = CatBoostClassifier(verbose=False,\n",
    "                             task_type=\"GPU\",\n",
    "                             loss_function=\"Logloss\",\n",
    "                             eval_metric=\"Logloss\",\n",
    "                             **best_params,)\n",
    "\n",
    "# Data\n",
    "data_train = ParticleDataset()\n",
    "data_test = ParticleDataset(path=f\"{APP_ML_PATH}/data/initial/train\",\n",
    "                            target=\"ALL\")\n",
    "data_train[:][0].to(\"cpu\")\n",
    "data_train[:][1].to(\"cpu\")\n",
    "data_test = data_test[:][0].to(\"cpu\")\n",
    "\n",
    "\n",
    "lgbm.fit(data_train[:][0].detach().numpy(), data_train[:][1].detach().numpy())\n",
    "cat_clf.fit(data_train[:][0].detach().numpy(), data_train[:][1].detach().numpy())\n",
    "\n",
    "lgbm_proba = lgbm.predict_proba(data_test.detach().numpy())[:, 1]\n",
    "cat_proba = cat_clf.predict_proba(data_test.detach().numpy())[:, 1]\n",
    "nn_proba = model1(data_test).detach().numpy().reshape(-1)\n",
    "print(lgbm_proba.shape)\n",
    "print(cat_proba.shape)\n",
    "print(nn_proba.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-19T23:04:15.742357Z",
     "end_time": "2023-05-19T23:04:33.310871Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9650276923076923\n",
      "0.9550892307692308\n",
      "0.9270338461538462\n"
     ]
    }
   ],
   "source": [
    "# Create ensembles\n",
    "w1 = lgbm.score(data_train[:][0].detach().numpy(), data_train[:][1].numpy())\n",
    "print(w1)\n",
    "w2 = cat_clf.score(data_train[:][0].detach().numpy(), data_train[:][1].numpy())\n",
    "print(w2)\n",
    "w3 = np.mean(np.round(model1(data_train[:][0]).detach().numpy().reshape(-1)) == data_train[:][1].numpy())\n",
    "print(w3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-19T23:04:33.315007Z",
     "end_time": "2023-05-19T23:04:35.287513Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "ensemble_avg = (lgbm_proba + cat_proba + nn_proba) / 3\n",
    "ensemble_avg_w = (w1 * lgbm_proba + w2 * cat_proba + w3 * nn_proba) / (w1 + w2 + w3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-19T23:04:35.290015Z",
     "end_time": "2023-05-19T23:04:35.292853Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "lgbm_results = pd.Series(lgbm_proba)\n",
    "cat_results = pd.Series(cat_proba)\n",
    "nn_results = pd.Series(nn_proba)\n",
    "ensemble_avg_results = pd.Series(ensemble_avg)\n",
    "ensemble_wavg_results = pd.Series(ensemble_avg_w)\n",
    "\n",
    "lgbm_results.to_csv(f\"{APP_ML_PATH}/data/initial/solutions/Classification_AndreasMHermansen_LGBM.txt\")\n",
    "cat_results.to_csv(f\"{APP_ML_PATH}/data/initial/solutions/Classification_AndreasMHermansen_Catboost.txt\")\n",
    "nn_results.to_csv(f\"{APP_ML_PATH}/data/initial/solutions/Classification_AndreasMHermansen_NeuralNet.txt\")\n",
    "ensemble_avg_results.to_csv(f\"{APP_ML_PATH}/data/initial/solutions/Classification_AndreasMHermansen_3ModelLinearEnsemble.txt\")\n",
    "ensemble_wavg_results.to_csv(f\"{APP_ML_PATH}/data/initial/solutions/Classification_AndreasMHermansen_3ModelWeightedLinearEnsemble.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-19T23:25:16.017608Z",
     "end_time": "2023-05-19T23:25:18.121101Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

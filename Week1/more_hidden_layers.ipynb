{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-29T20:29:15.694869Z",
     "end_time": "2023-04-29T20:29:15.700385Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from time import perf_counter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0.0718, -0.5072, -0.8899, -0.9791,  1.1071,  0.2711], device='cuda:0'), tensor([0., 1.], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "class AlephDataset(Dataset):\n",
    "    def __init__(self, file_path='AlephBtag_MC_train_Nev50000.csv', device='cuda'):\n",
    "        self.device = device\n",
    "        # Get data (with this very useful NumPy reader):\n",
    "        data = np.genfromtxt(file_path, names=True)  # For faster running\n",
    "        # data = np.genfromtxt('AlephBtag_MC_train_Nev50000.csv', names=True)   # For more data\n",
    "\n",
    "        # Kinematics (energy and direction) of the jet:\n",
    "        energy = data['energy']\n",
    "        cTheta = data['cTheta']\n",
    "        phi    = data['phi']\n",
    "\n",
    "        # Classification variables (those used in Aleph's NN):\n",
    "        prob_b = data['prob_b']\n",
    "        spheri = data['spheri']\n",
    "        pt2rel = data['pt2rel']\n",
    "        multip = data['multip']\n",
    "        bqvjet = data['bqvjet']\n",
    "        ptlrel = data['ptlrel']\n",
    "\n",
    "        # Aleph's NN score:\n",
    "        nnbjet = data['nnbjet']\n",
    "\n",
    "        # Truth variable whether it really was a b-jet or not (i.e. target)\n",
    "        isb    = data['isb']\n",
    "\n",
    "        features = pd.DataFrame({\n",
    "            \"prob_b\": prob_b,\n",
    "            \"spheri\": spheri,\n",
    "            \"pt2rel\": pt2rel,\n",
    "            \"multip\": multip,\n",
    "            \"bqvjet\": bqvjet,\n",
    "            \"ptlrel\": ptlrel,\n",
    "        })\n",
    "\n",
    "        labels = pd.Series(isb, dtype='int16')\n",
    "        self.features = features.to_numpy()\n",
    "        self.features = (self.features - np.mean(self.features, axis=0)) / np.std(self.features, axis=0)\n",
    "        self.labels = labels.to_numpy(dtype=np.int8)\n",
    "        self.labels = np.stack([self.labels, (1 - self.labels)])\n",
    "        self.labels = self.labels.T\n",
    "        self.labels.flatten('C')\n",
    "\n",
    "        self.features = torch.Tensor(self.features).to(self.device)\n",
    "        self.labels = torch.Tensor(self.labels).to(self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx, :], self.labels[idx]\n",
    "\n",
    "\n",
    "aleph_train_data = AlephDataset('../data/AlephBtag_MC_train_Nev5000000.csv', device=torch.device('cuda:0'))\n",
    "aleph_valid_data = AlephDataset('AlephBtag_MC_train_Nev50000.csv', device=torch.device('cuda:0'))\n",
    "\n",
    "print(aleph_train_data[55])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T20:43:14.610031Z",
     "end_time": "2023-04-29T20:43:40.959687Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "VALID_BATCH_SIZE = 50000\n",
    "TRAIN_BATCH_SIZE = 250000\n",
    "training_loader = DataLoader(aleph_train_data, shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "validation_loader = DataLoader(aleph_valid_data, shuffle=False, batch_size=VALID_BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T20:38:56.854534Z",
     "end_time": "2023-04-29T20:38:56.899808Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class FullyConnectedBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=15, p_dropout=0.25, activation=nn.LeakyReLU()):\n",
    "        super().__init__()\n",
    "        layer = nn.Linear(in_channels, out_channels)\n",
    "        drop = nn.Dropout(p=p_dropout)\n",
    "        activation = activation\n",
    "        self.model = nn.Sequential(layer, drop, activation)\n",
    "        self.skip_connection = in_channels == out_channels\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if self.skip_connection:\n",
    "            out += x\n",
    "        return out\n",
    "\n",
    "\n",
    "class FirstModel(nn.Module):\n",
    "    def __init__(self, in_channels=6, hidden_channels=15, decode_channels=6, out_channels=2, n_layers=4, p_dropout=0.25, activation=nn.LeakyReLU()):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(in_channels, hidden_channels), nn.LeakyReLU()]\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(FullyConnectedBlock(hidden_channels, hidden_channels, p_dropout=p_dropout, activation=activation))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.decoder = nn.Sequential(*[nn.Linear(hidden_channels, decode_channels),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                       nn.Linear(decode_channels, out_channels),\n",
    "                                       nn.Softmax(out_channels)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "\n",
    "model = FirstModel(hidden_channels=25, n_layers=4).to(torch.device('cuda:0'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T20:29:04.906509Z",
     "end_time": "2023-04-29T20:29:04.952977Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, loss_fn, model, optimizer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / TRAIN_BATCH_SIZE  # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T20:15:22.561250Z",
     "end_time": "2023-04-29T20:15:22.564333Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amh/Documents/coding/GitHub/AppliedML2023/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 10 loss: 0.00013479848623275758\n",
      "  batch 20 loss: 0.00011892963290214539\n",
      "LOSS train 0.00011892963290214539 Validation accuracy: 0.776199996471405\n",
      "Epoch time: 242.1325457999992s\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.00010784187316894531\n",
      "  batch 20 loss: 9.923657417297364e-05\n",
      "LOSS train 9.923657417297364e-05 Validation accuracy: 0.7762599587440491\n",
      "Epoch time: 242.83863103900148s\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 9.42753541469574e-05\n",
      "  batch 20 loss: 9.161818623542785e-05\n",
      "LOSS train 9.161818623542785e-05 Validation accuracy: 0.8947399854660034\n",
      "Epoch time: 244.14146987599815s\n",
      "EPOCH 4:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001B[39;00m\n\u001B[1;32m     26\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 27\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_number\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# We don't need gradients on to do reporting\u001B[39;00m\n\u001B[1;32m     30\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[7], line 8\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(epoch_index, tb_writer, loss_fn, model, optimizer)\u001B[0m\n\u001B[1;32m      3\u001B[0m last_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Here, we use enumerate(training_loader) instead of\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# iter(training_loader) so that we can track the batch\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# index and do some intra-epoch reporting\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(training_loader):\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m# Every data instance is an input + label pair\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     inputs, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;66;03m# Zero your gradients for every batch!\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/coding/GitHub/AppliedML2023/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Documents/coding/GitHub/AppliedML2023/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    677\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 678\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    680\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Documents/coding/GitHub/AppliedML2023/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Documents/coding/GitHub/AppliedML2023/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[17], line 48\u001B[0m, in \u001B[0;36mAlephDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[0;32m---> 48\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mTensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures[idx, :])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice), \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "NEW_MODEL = True\n",
    "if NEW_MODEL:\n",
    "    model = FirstModel(hidden_channels=25, n_layers=6, p_dropout=0.05, activation=nn.LeakyReLU())\n",
    "model.to(device=torch.device('cuda:0'))\n",
    "print(\"Starting\")\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 25\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "Path(\"models\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = perf_counter()\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer, loss_fn, model=model, optimizer=optimizer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_acc = 0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "        running_acc += torch.sum(torch.argmax(voutputs, axis=1) == torch.argmax(vlabels, axis=1))\n",
    "    running_acc = running_acc / ((i + 1) * VALID_BATCH_SIZE)\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS train {avg_loss} Validation accuracy: {running_acc}')\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'models/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    end_time = perf_counter()\n",
    "    print(f\"Epoch time: {end_time-start_time}s\")\n",
    "    epoch_number += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T19:10:17.500339Z",
     "end_time": "2023-04-26T19:13:35.974113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-26T22:05:12.396537Z",
     "end_time": "2023-04-26T22:05:18.386438Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 22:05:15.061762: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-26 22:05:15.611036: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-26 22:05:15.611066: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-26 22:05:16.993116: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-26 22:05:16.993854: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-26 22:05:16.993871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AlephDataset(Dataset):\n",
    "    def __init__(self, file_path='AlephBtag_MC_train_Nev50000.csv'):\n",
    "        # Get data (with this very useful NumPy reader):\n",
    "        data = np.genfromtxt(file_path, names=True)  # For faster running\n",
    "        # data = np.genfromtxt('AlephBtag_MC_train_Nev50000.csv', names=True)   # For more data\n",
    "\n",
    "        # Kinematics (energy and direction) of the jet:\n",
    "        energy = data['energy']\n",
    "        cTheta = data['cTheta']\n",
    "        phi    = data['phi']\n",
    "\n",
    "        # Classification variables (those used in Aleph's NN):\n",
    "        prob_b = data['prob_b']\n",
    "        spheri = data['spheri']\n",
    "        pt2rel = data['pt2rel']\n",
    "        multip = data['multip']\n",
    "        bqvjet = data['bqvjet']\n",
    "        ptlrel = data['ptlrel']\n",
    "\n",
    "        # Aleph's NN score:\n",
    "        nnbjet = data['nnbjet']\n",
    "\n",
    "        # Truth variable whether it really was a b-jet or not (i.e. target)\n",
    "        isb    = data['isb']\n",
    "\n",
    "        features = pd.DataFrame({\n",
    "            \"prob_b\": prob_b,\n",
    "            \"spheri\": spheri,\n",
    "            \"pt2rel\": pt2rel,\n",
    "            \"multip\": multip,\n",
    "            \"bqvjet\": bqvjet,\n",
    "            \"ptlrel\": ptlrel,\n",
    "        })\n",
    "\n",
    "        labels = pd.Series(isb, dtype='int16')\n",
    "        self.features = features.to_numpy()\n",
    "        self.features = (self.features - np.mean(self.features, axis=0)) / np.std(self.features, axis=0)\n",
    "        self.labels = labels.to_numpy(dtype=np.int8)\n",
    "        self.labels = np.stack([self.labels, (1 - self.labels)])\n",
    "        self.labels = self.labels.T\n",
    "        self.labels.flatten('C')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.features[idx, :]), torch.Tensor([self.labels[idx]]).flatten()\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2500\n",
    "aleph_train_data = AlephDataset('../data/AlephBtag_MC_train_Nev5000000.csv')\n",
    "aleph_valid_data = AlephDataset('AlephBtag_MC_train_Nev50000.csv')\n",
    "training_loader = DataLoader(aleph_train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "validation_loader = DataLoader(aleph_valid_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(aleph_train_data[55])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T19:04:48.900371Z",
     "end_time": "2023-04-26T19:04:55.834136Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FullyConnectedBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=15, p_dropout=0.25, activation=nn.LeakyReLU()):\n",
    "        super().__init__()\n",
    "        layer = nn.Linear(in_channels, out_channels)\n",
    "        drop = nn.Dropout(p=p_dropout)\n",
    "        activation = activation\n",
    "        self.model = nn.Sequential(layer, drop, activation)\n",
    "        self.skip_connection = in_channels == out_channels\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        if self.skip_connection:\n",
    "            out += x\n",
    "        return out\n",
    "\n",
    "\n",
    "class FirstModel(nn.Module):\n",
    "    def __init__(self, in_channels=6, hidden_channels=15, decode_channels=6, out_channels=2, n_layers=4, p_dropout=0.25, activation=nn.LeakyReLU()):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(in_channels, hidden_channels), nn.LeakyReLU()]\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(FullyConnectedBlock(hidden_channels, hidden_channels, p_dropout=p_dropout, activation=activation))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.decoder = nn.Sequential(*[nn.Linear(hidden_channels, decode_channels),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                       nn.Linear(decode_channels, out_channels),\n",
    "                                       nn.Softmax()])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out\n",
    "\n",
    "model = FirstModel(hidden_channels=15, n_layers=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T19:07:47.338016Z",
     "end_time": "2023-04-26T19:07:47.382498Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, loss_fn, model, optimizer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / BATCH_SIZE  # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T19:05:14.681871Z",
     "end_time": "2023-04-26T19:05:14.723702Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "NEW_MODEL = False\n",
    "if NEW_MODEL:\n",
    "    model = FirstModel(hidden_channels=25, n_layers=6, p_dropout=0.05, activation=nn.LeakyReLU())\n",
    "print(\"Starting\")\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "Path(\"models\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer, loss_fn, model=model, optimizer=optimizer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_acc = 0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "        running_acc += torch.sum(torch.argmax(voutputs, axis=1) == torch.argmax(vlabels, axis=1))\n",
    "    running_acc = running_acc / ((i + 1) * BATCH_SIZE)\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(f'LOSS train {avg_loss} Validation accuracy: {running_acc}')\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'models/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T19:10:17.500339Z",
     "end_time": "2023-04-26T19:13:35.974113Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
